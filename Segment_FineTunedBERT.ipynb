{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning BERT for Segment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "excel_file = './Segment_Training_Data.xlsx'\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "descriptions = df['Descriptions'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Tokenization of Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenized_texts = tokenizer(descriptions, truncation=True, padding=True)\n",
    "\n",
    "combined_texts = [{'input_ids': input_ids, 'attention_mask': attention_mask} \n",
    "                  for input_ids, attention_mask in zip(tokenized_texts['input_ids'], tokenized_texts['attention_mask'])]\n",
    "\n",
    "labels = list(df['Segment'])\n",
    "\n",
    "label_map = {label: idx for idx, label in enumerate(set(labels))}\n",
    "labels = [label_map[label] for label in labels]\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(combined_texts, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': torch.tensor(self.encodings[idx]['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.encodings[idx]['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = CustomDataset(train_texts, train_labels)\n",
    "val_dataset = CustomDataset(val_texts, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Training batch 1 last loss: 0.16744795441627502\n",
      "Training batch 2 last loss: 0.17218975722789764\n",
      "Training batch 3 last loss: 0.17828239500522614\n",
      "Training batch 4 last loss: 0.18215349316596985\n",
      "Training batch 5 last loss: 0.18159803748130798\n",
      "Training batch 6 last loss: 0.178180992603302\n",
      "Training batch 7 last loss: 0.18475373089313507\n",
      "Training batch 8 last loss: 0.1714879423379898\n",
      "Training batch 9 last loss: 0.17255258560180664\n",
      "Training batch 10 last loss: 0.17320516705513\n",
      "Training batch 11 last loss: 0.19088587164878845\n",
      "Training batch 12 last loss: 0.16198104619979858\n",
      "Training batch 13 last loss: 0.17010800540447235\n",
      "Training batch 14 last loss: 0.17044198513031006\n",
      "Training batch 15 last loss: 0.17426776885986328\n",
      "Training batch 16 last loss: 0.1722099334001541\n",
      "Training batch 17 last loss: 0.1711837202310562\n",
      "\n",
      "Training epoch 1 loss:  0.1711837202310562\n",
      "Epoch 1, Validation Loss: 2.6990736961364745, Validation Accuracy: 7.462686567164178%\n",
      "Epoch:  2\n",
      "Training batch 1 last loss: 0.1689862459897995\n",
      "Training batch 2 last loss: 0.1761702597141266\n",
      "Training batch 3 last loss: 0.16396379470825195\n",
      "Training batch 4 last loss: 0.16583050787448883\n",
      "Training batch 5 last loss: 0.17003396153450012\n",
      "Training batch 6 last loss: 0.16951921582221985\n",
      "Training batch 7 last loss: 0.16996441781520844\n",
      "Training batch 8 last loss: 0.1602887362241745\n",
      "Training batch 9 last loss: 0.16398769617080688\n",
      "Training batch 10 last loss: 0.15932048857212067\n",
      "Training batch 11 last loss: 0.15998715162277222\n",
      "Training batch 12 last loss: 0.152696430683136\n",
      "Training batch 13 last loss: 0.15691505372524261\n",
      "Training batch 14 last loss: 0.1553603708744049\n",
      "Training batch 15 last loss: 0.15931105613708496\n",
      "Training batch 16 last loss: 0.14495210349559784\n",
      "Training batch 17 last loss: 0.1560429334640503\n",
      "\n",
      "Training epoch 2 loss:  0.1560429334640503\n",
      "Epoch 2, Validation Loss: 2.356559324264526, Validation Accuracy: 29.850746268656714%\n",
      "Epoch:  3\n",
      "Training batch 1 last loss: 0.14517930150032043\n",
      "Training batch 2 last loss: 0.14611469209194183\n",
      "Training batch 3 last loss: 0.14570356905460358\n",
      "Training batch 4 last loss: 0.14517371356487274\n",
      "Training batch 5 last loss: 0.14397664368152618\n",
      "Training batch 6 last loss: 0.15109965205192566\n",
      "Training batch 7 last loss: 0.13195499777793884\n",
      "Training batch 8 last loss: 0.14110040664672852\n",
      "Training batch 9 last loss: 0.14352409541606903\n",
      "Training batch 10 last loss: 0.14849382638931274\n",
      "Training batch 11 last loss: 0.13173317909240723\n",
      "Training batch 12 last loss: 0.13233715295791626\n",
      "Training batch 13 last loss: 0.13934481143951416\n",
      "Training batch 14 last loss: 0.13535358011722565\n",
      "Training batch 15 last loss: 0.13093821704387665\n",
      "Training batch 16 last loss: 0.12505406141281128\n",
      "Training batch 17 last loss: 0.1435377448797226\n",
      "\n",
      "Training epoch 3 loss:  0.1435377448797226\n",
      "Epoch 3, Validation Loss: 1.928433609008789, Validation Accuracy: 64.17910447761194%\n",
      "Epoch:  4\n",
      "Training batch 1 last loss: 0.11984480172395706\n",
      "Training batch 2 last loss: 0.12598000466823578\n",
      "Training batch 3 last loss: 0.11811237037181854\n",
      "Training batch 4 last loss: 0.11427287012338638\n",
      "Training batch 5 last loss: 0.12687233090400696\n",
      "Training batch 6 last loss: 0.1299448013305664\n",
      "Training batch 7 last loss: 0.10990907996892929\n",
      "Training batch 8 last loss: 0.11489711701869965\n",
      "Training batch 9 last loss: 0.09870772063732147\n",
      "Training batch 10 last loss: 0.11238493025302887\n",
      "Training batch 11 last loss: 0.10199189186096191\n",
      "Training batch 12 last loss: 0.09806305915117264\n",
      "Training batch 13 last loss: 0.11135371774435043\n",
      "Training batch 14 last loss: 0.11492718011140823\n",
      "Training batch 15 last loss: 0.10851728916168213\n",
      "Training batch 16 last loss: 0.09993556141853333\n",
      "Training batch 17 last loss: 0.0842989906668663\n",
      "\n",
      "Training epoch 4 loss:  0.0842989906668663\n",
      "Epoch 4, Validation Loss: 1.5472204685211182, Validation Accuracy: 82.08955223880598%\n",
      "Epoch:  5\n",
      "Training batch 1 last loss: 0.09407170116901398\n",
      "Training batch 2 last loss: 0.10129006952047348\n",
      "Training batch 3 last loss: 0.10020501166582108\n",
      "Training batch 4 last loss: 0.09104675054550171\n",
      "Training batch 5 last loss: 0.08975455164909363\n",
      "Training batch 6 last loss: 0.0954115241765976\n",
      "Training batch 7 last loss: 0.08886399120092392\n",
      "Training batch 8 last loss: 0.08271950483322144\n",
      "Training batch 9 last loss: 0.08374449610710144\n",
      "Training batch 10 last loss: 0.09220673888921738\n",
      "Training batch 11 last loss: 0.08224406838417053\n",
      "Training batch 12 last loss: 0.0826646164059639\n",
      "Training batch 13 last loss: 0.07662725448608398\n",
      "Training batch 14 last loss: 0.07216393947601318\n",
      "Training batch 15 last loss: 0.07312221080064774\n",
      "Training batch 16 last loss: 0.08582809567451477\n",
      "Training batch 17 last loss: 0.07613429427146912\n",
      "\n",
      "Training epoch 5 loss:  0.07613429427146912\n",
      "Epoch 5, Validation Loss: 1.1564929962158204, Validation Accuracy: 89.55223880597015%\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_map))\n",
    "\n",
    "# Set device (GPU/CPU)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer, Loss function\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # Adjust number of epochs as needed\n",
    "    print(\"Epoch: \",(epoch + 1))\n",
    "    model.train()\n",
    "    for i,batch in enumerate(train_loader): \n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        pred = outputs.logits\n",
    "        loss = loss_fn(pred, batch['labels'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_batch_loss = loss.item()\n",
    "        train_last_loss = train_batch_loss / 16\n",
    "        print('Training batch {} last loss: {}'.format(i + 1, train_last_loss))\n",
    "    print(f\"\\nTraining epoch {epoch + 1} loss: \",train_last_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, Validation Loss: {val_loss / len(val_loader)}, Validation Accuracy: {(correct / total) * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Infrastructure']\n"
     ]
    }
   ],
   "source": [
    "new_texts = [\n",
    "    \"\"\"\n",
    "    ICT Service Desk Manager EUMETSAT is Europe ’ s meteorological satellite agency - monitoring the weather and climate from space - 24 hours a day , 365 days a year . Working for EUMETSAT , \n",
    "    you can make a world of difference and be a part of something that makes a positive impact on society . You will be at the cutting edge of satellite technology , \n",
    "    with a meaningful role in an organisation focused on space - based observations of the Earth ’ s weather and climate . In the EUMETSAT matrix organisation , \n",
    "    the Information and Communication Technology ( ICT ) Division is responsible for providing applications and support services regarding Information and Communication Technology to the organisation . The ICT Division is a dynamic team of more than 50 technicians and engineers , \n",
    "    which operates , manages , troubleshoots and implements changes to corporate ICT systems , including desktop and mobile IT equipment , SAP , Documentation Management Tool , EUMETSAT web sites and the intranet . In the EUMETSAT matrix organisation , the Information and Communication Technology ( ICT ) Division is responsible for providing applications and support services regarding Informationand Communication Technology to the organisation . \n",
    "    The ICT Division is a dynamic team of more than 50 technicians and engineers , which operates , manages , troubleshoots andimplements changes to corporate ICT systems , \n",
    "    including desktop and mobile IT equipment , SAP , Documentation Management Tool , EUMETSAT web sites and the intranet . As the ICT Service Desk Manager , \n",
    "    you will play a pivotal role in ensuring the smooth operation of our service provision . Your responsibilities will include maintaining service quality and ensuring user satisfaction . \n",
    "    With EUMETSAT embarking on an exciting phase marked by multiple upcoming satellite launches , joining our multi - cultural team presents both challenges and opportunities for personal and professional growth . \n",
    "    What you ’ ll be doing : Under the direct supervision of the ICT Service Delivery Manager and working within the matrix structure of the ICT Division , the Service Desk Manager will be responsible for : Operate the Service Desk , including management of a team of 7 technicians . \n",
    "    Coordinate and implement IT Incident Management and Change Management processes , adhering to existing Service Level Agreements . Ensure timely communication with users and management , \n",
    "    and handle service requests . Provide user support for all IT services and contribute to technical support within the team . Document the team ’ s technical knowledge . \n",
    "    Procure and manage end - user IT equipment ( laptops , phones ) and shared equipment ( corridor printers , meeting room devices ) . Maintain an up - to - date inventory of ICT equipment . \n",
    "    Assist with large deployments of software and devices , including relevant end - user communication . Advise on overall strategies for user support , productivity , roll - out projects , \n",
    "    and training needs . Act as a deputy for the ICT Service Delivery Manager . What we offer : Excellent salary , of up to Euro 8000 NET ( after tax ) based on skills and experience ; \n",
    "    Flexible working time including additional flexi - leave ; Full medical coverage for employee and family ; Attractive pension ; 30 days of annual leave + 14 . 5 days public holidays ; \n",
    "    Training and development support ; Relocation allowance and support ( if applicable ) . \n",
    "    Requirements : Qualifications : Completed secondary education and possess appropriate professional qualifications . \n",
    "    Skills and Experience Requirements : Minimum five years experience in managing IT user support and helpdesk teams . Experience in IT system and application user support and administration . \n",
    "    Extensive experience in supporting and interacting with demanding stakeholders , customers and users of IT Services . Strong customer focus . Strong interpersonal skills , \n",
    "    with proven ability to apply these to interact with management and working within , and across , teams . Flexibility to adapt to changing organisational priorities and user needs . \n",
    "    Knowledge of ISO 9000 and ITIL , as well as knowledge and hands - on experience with the following are desirable : Microsoft 365 , Atlassian Jira & Confluence . \n",
    "    Languages : Candidates need to be able to work effectively in English More about us : EUMETSAT ’ s role is to establish and operate meteorological satellites to monitor the weather and climate from space - 24 hours a day , 365 days a year . \n",
    "    This information is supplied to the National Meteorological Services of the organisation ' s Member and Cooperating States in Europe , as well as other users worldwide . \n",
    "    EUMETSAT also operates several Copernicus missions on behalf of the European Union and provide data services to the Copernicus marine and atmospheric services and their users . \n",
    "    As an intergovernmental European Organisation , EUMETSAT can recruit nationals only from the 30 Member States ( Austria , Belgium , Bulgaria , Croatia , Czech Republic , Denmark , Estonia , Finland , France , Germany , Greece , Hungary , Iceland , Ireland , Italy , Latvia , Lithuania , Luxembourg , The Netherlands , Norway , Poland , Portugal , Romania , Slovakia , Slovenia , Spain , Sweden , Switzerland , Turkey and the United Kingdom ) . \n",
    "    Show more Show less Information Technology Defense and Space Manufacturing\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(new_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "# Realizar predicciones\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Obtener las predicciones de clase\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Convertir las predicciones de índices a etiquetas (si se desea)\n",
    "predicted_labels = [list(label_map.keys())[list(label_map.values()).index(pred)] for pred in predictions]\n",
    "\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['segment_model.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Guardar el modelo\n",
    "joblib.dump(model, 'segment_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
