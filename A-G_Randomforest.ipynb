{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aeFuMV21s2M2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9666666666666667\n",
            "Precision: 1.0\n",
            "Recall: 0.9375\n",
            "F1-score: 0.967741935483871\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "import numpy as np\n",
        "from gensim import downloader as api\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "df = pd.read_excel('TrainingEminDataSet.xlsx')\n",
        "\n",
        "columns = ['Title', 'OfferDescription', 'Requirements', 'Responsibilities', 'AdditionalInformation', 'Descriptions']\n",
        "existing_columns = [col for col in columns if col in df.columns]\n",
        "\n",
        "\n",
        "# Combine relevant columns into a single text column, handling missing values\n",
        "df['combined_text'] = df[existing_columns].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
        "\n",
        "# Handle empty combined texts\n",
        "df['combined_text'] = df['combined_text'].apply(lambda x: x if x else 'NA')\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['lemmatized_text'] = df['combined_text'].apply(lambda text: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(text)]))\n",
        "\n",
        "# Word2Vec embeddings\n",
        "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "def sentence_to_vec(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    vectors = [word2vec_model[word] for word in words if word in word2vec_model.key_to_index]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(300)\n",
        "\n",
        "df['word2vec'] = df['lemmatized_text'].apply(sentence_to_vec)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['word2vec'].tolist(), df['Label'], test_size=0.2, random_state=42)\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Tahminler ve deÄŸerlendirme\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
