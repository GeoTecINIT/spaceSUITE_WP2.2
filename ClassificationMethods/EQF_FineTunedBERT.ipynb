{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning BERT for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "excel_file = './Euraxess_GNSS_Keywords.xlsx'\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "df['Concatenated'] = df[['Title', 'OfferDescription', 'Requirements', 'AdditionalInformation']].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
    "\n",
    "descriptions = df['Concatenated'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Tokenization of Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenized_texts = tokenizer(descriptions, truncation=True, padding=True)\n",
    "\n",
    "combined_texts = [{'input_ids': input_ids, 'attention_mask': attention_mask} \n",
    "                  for input_ids, attention_mask in zip(tokenized_texts['input_ids'], tokenized_texts['attention_mask'])]\n",
    "\n",
    "labels = list(df['Position'])\n",
    "\n",
    "label_map = {label: idx for idx, label in enumerate(set(labels))}\n",
    "labels = [label_map[label] for label in labels]\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(combined_texts, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': torch.tensor(self.encodings[idx]['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.encodings[idx]['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = CustomDataset(train_texts, train_labels)\n",
    "val_dataset = CustomDataset(val_texts, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Training batch 1 last loss: 0.14404307305812836\n",
      "Training batch 2 last loss: 0.11648021638393402\n",
      "Training batch 3 last loss: 0.11102175712585449\n",
      "Training batch 4 last loss: 0.11154456436634064\n",
      "Training batch 5 last loss: 0.10801249742507935\n",
      "Training batch 6 last loss: 0.10873870551586151\n",
      "Training batch 7 last loss: 0.10070991516113281\n",
      "Training batch 8 last loss: 0.10630971938371658\n",
      "Training batch 9 last loss: 0.10036035627126694\n",
      "Training batch 10 last loss: 0.093991719186306\n",
      "Training batch 11 last loss: 0.08659720420837402\n",
      "Training batch 12 last loss: 0.08865533024072647\n",
      "Training batch 13 last loss: 0.08101295679807663\n",
      "Training batch 14 last loss: 0.08280676603317261\n",
      "Training batch 15 last loss: 0.07919792085886002\n",
      "Training batch 16 last loss: 0.08716821670532227\n",
      "Training batch 17 last loss: 0.07308534532785416\n",
      "Training batch 18 last loss: 0.07089197635650635\n",
      "Training batch 19 last loss: 0.06962834298610687\n",
      "Training batch 20 last loss: 0.06647060066461563\n",
      "Training batch 21 last loss: 0.0681440681219101\n",
      "Training batch 22 last loss: 0.07260462641716003\n",
      "Training batch 23 last loss: 0.08823394030332565\n",
      "Training batch 24 last loss: 0.05918903276324272\n",
      "Training batch 25 last loss: 0.07384172081947327\n",
      "Training batch 26 last loss: 0.05263257026672363\n",
      "Training batch 27 last loss: 0.09839276224374771\n",
      "Training batch 28 last loss: 0.06605710089206696\n",
      "Training batch 29 last loss: 0.082045778632164\n",
      "Training batch 30 last loss: 0.053241949528455734\n",
      "Training batch 31 last loss: 0.054985515773296356\n",
      "Training batch 32 last loss: 0.06853143125772476\n",
      "Training batch 33 last loss: 0.03877628967165947\n",
      "Training batch 34 last loss: 0.06450524181127548\n",
      "Training batch 35 last loss: 0.047358084470033646\n",
      "Training batch 36 last loss: 0.07866036146879196\n",
      "Training batch 37 last loss: 0.042201846837997437\n",
      "Training batch 38 last loss: 0.06158435717225075\n",
      "Training batch 39 last loss: 0.07382351905107498\n",
      "Training batch 40 last loss: 0.05080151557922363\n",
      "Training batch 41 last loss: 0.05406244099140167\n",
      "Training batch 42 last loss: 0.05447187274694443\n",
      "Training batch 43 last loss: 0.04173070192337036\n",
      "Training batch 44 last loss: 0.0637507289648056\n",
      "Training batch 45 last loss: 0.05580271780490875\n",
      "Training batch 46 last loss: 0.06957247108221054\n",
      "Training batch 47 last loss: 0.041428092867136\n",
      "Training batch 48 last loss: 0.04544515535235405\n",
      "Training batch 49 last loss: 0.04208563640713692\n",
      "Training batch 50 last loss: 0.048958100378513336\n",
      "Training batch 51 last loss: 0.029452018439769745\n",
      "Training batch 52 last loss: 0.0345727764070034\n",
      "Training batch 53 last loss: 0.05705980956554413\n",
      "Training batch 54 last loss: 0.05222470313310623\n",
      "Training batch 55 last loss: 0.03640373796224594\n",
      "Training batch 56 last loss: 0.06775124371051788\n",
      "Training batch 57 last loss: 0.03999689593911171\n",
      "Training batch 58 last loss: 0.044610440731048584\n",
      "Training batch 59 last loss: 0.058630287647247314\n",
      "Training batch 60 last loss: 0.040448956191539764\n",
      "Training batch 61 last loss: 0.07092797756195068\n",
      "Training batch 62 last loss: 0.04438197612762451\n",
      "Training batch 63 last loss: 0.05241881310939789\n",
      "Training batch 64 last loss: 0.04981943219900131\n",
      "Training batch 65 last loss: 0.03670555353164673\n",
      "Training batch 66 last loss: 0.05589108541607857\n",
      "Training batch 67 last loss: 0.049448441714048386\n",
      "Training batch 68 last loss: 0.07291366904973984\n",
      "Training batch 69 last loss: 0.03922908380627632\n",
      "Training batch 70 last loss: 0.0653490275144577\n",
      "Training batch 71 last loss: 0.021903008222579956\n",
      "Training batch 72 last loss: 0.07260221987962723\n",
      "Training batch 73 last loss: 0.06311246752738953\n",
      "Training batch 74 last loss: 0.04305344074964523\n",
      "Training batch 75 last loss: 0.024996863678097725\n",
      "Training batch 76 last loss: 0.05743969976902008\n",
      "Training batch 77 last loss: 0.07418127357959747\n",
      "Training batch 78 last loss: 0.0800020843744278\n",
      "Training batch 79 last loss: 0.05800621584057808\n",
      "Training batch 80 last loss: 0.05418317764997482\n",
      "Training batch 81 last loss: 0.05939821898937225\n",
      "Training batch 82 last loss: 0.032960444688797\n",
      "Training batch 83 last loss: 0.06077500432729721\n",
      "Training batch 84 last loss: 0.052837904542684555\n",
      "Training batch 85 last loss: 0.047365181148052216\n",
      "Training batch 86 last loss: 0.05413122847676277\n",
      "Training batch 87 last loss: 0.044098589569330215\n",
      "Training batch 88 last loss: 0.043954551219940186\n",
      "Training batch 89 last loss: 0.04477420076727867\n",
      "Training batch 90 last loss: 0.0672018975019455\n",
      "Training batch 91 last loss: 0.04798610135912895\n",
      "Training batch 92 last loss: 0.0627058669924736\n",
      "Training batch 93 last loss: 0.05377596989274025\n",
      "Training batch 94 last loss: 0.04751235619187355\n",
      "Training batch 95 last loss: 0.06469495594501495\n",
      "Training batch 96 last loss: 0.048014167696237564\n",
      "Training batch 97 last loss: 0.03676905855536461\n",
      "Training batch 98 last loss: 0.04313526675105095\n",
      "Training batch 99 last loss: 0.049264028668403625\n",
      "Training batch 100 last loss: 0.04045840725302696\n",
      "Training batch 101 last loss: 0.03108779340982437\n",
      "Training batch 102 last loss: 0.04653859883546829\n",
      "Training batch 103 last loss: 0.03265327215194702\n",
      "Training batch 104 last loss: 0.0571233294904232\n",
      "Training batch 105 last loss: 0.0399695560336113\n",
      "Training batch 106 last loss: 0.041520822793245316\n",
      "Training batch 107 last loss: 0.0452130101621151\n",
      "Training batch 108 last loss: 0.03557605296373367\n",
      "Training batch 109 last loss: 0.04063306003808975\n",
      "Training batch 110 last loss: 0.026089070364832878\n",
      "Training batch 111 last loss: 0.07536578178405762\n",
      "Training batch 112 last loss: 0.038146067410707474\n",
      "Training batch 113 last loss: 0.055880095809698105\n",
      "Training batch 114 last loss: 0.04774399846792221\n",
      "Training batch 115 last loss: 0.038318879902362823\n",
      "Training batch 116 last loss: 0.047598063945770264\n",
      "Training batch 117 last loss: 0.05236876383423805\n",
      "Training batch 118 last loss: 0.0461701974272728\n",
      "Training batch 119 last loss: 0.07446786761283875\n",
      "Training batch 120 last loss: 0.03314167261123657\n",
      "Training batch 121 last loss: 0.054898884147405624\n",
      "Training batch 122 last loss: 0.04541178047657013\n",
      "Training batch 123 last loss: 0.059892166405916214\n",
      "Training batch 124 last loss: 0.04524041712284088\n",
      "Training batch 125 last loss: 0.06269258260726929\n",
      "Training batch 126 last loss: 0.040273066610097885\n",
      "Training batch 127 last loss: 0.028781097382307053\n",
      "Training batch 128 last loss: 0.05009886249899864\n",
      "Training batch 129 last loss: 0.0326162651181221\n",
      "Training batch 130 last loss: 0.061409275978803635\n",
      "Training batch 131 last loss: 0.046612028032541275\n",
      "Training batch 132 last loss: 0.04311731830239296\n",
      "Training batch 133 last loss: 0.05410129576921463\n",
      "Training batch 134 last loss: 0.04685698449611664\n",
      "Training batch 135 last loss: 0.0221538245677948\n",
      "Training batch 136 last loss: 0.06911949813365936\n",
      "Training batch 137 last loss: 0.026403451338410378\n",
      "Training batch 138 last loss: 0.042632631957530975\n",
      "Training batch 139 last loss: 0.041236478835344315\n",
      "Training batch 140 last loss: 0.05840396508574486\n",
      "Training batch 141 last loss: 0.06644662469625473\n",
      "Training batch 142 last loss: 0.0560634471476078\n",
      "Training batch 143 last loss: 0.06254892796278\n",
      "Training batch 144 last loss: 0.05229306221008301\n",
      "Training batch 145 last loss: 0.05752423033118248\n",
      "Training batch 146 last loss: 0.04787721857428551\n",
      "Training batch 147 last loss: 0.049587126821279526\n",
      "Training batch 148 last loss: 0.04064124822616577\n",
      "Training batch 149 last loss: 0.04216635972261429\n",
      "Training batch 150 last loss: 0.030989907681941986\n",
      "Training batch 151 last loss: 0.04467910900712013\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m pred \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     31\u001b[0m train_batch_loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_map))\n",
    "\n",
    "# Set device (GPU/CPU)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer, Loss function\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Adjust number of epochs as needed\n",
    "    print(\"Epoch: \",(epoch + 1))\n",
    "    model.train()\n",
    "    for i,batch in enumerate(train_loader): \n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        pred = outputs.logits\n",
    "        loss = loss_fn(pred, batch['labels'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_batch_loss = loss.item()\n",
    "        train_last_loss = train_batch_loss / 16\n",
    "        print('Training batch {} last loss: {}'.format(i + 1, train_last_loss))\n",
    "    print(f\"\\nTraining epoch {epoch + 1} loss: \",train_last_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, Validation Loss: {val_loss / len(val_loader)}, Validation Accuracy: {(correct / total) * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Bachelor's Degree\"]\n"
     ]
    }
   ],
   "source": [
    "new_texts = [\n",
    "    \"\"\"\"Offer Description\n",
    "        SuperGPS-2 projectThis research project aims to develop a robust and efficient terrestrial system for accurate positioning and time-transfer, using virtual ultra-wideband radio signals, which can serve as a backup and complement to GNSS (Global Navigation Satellite System) in environments with reduced GNSS availability. The virtual ultra-wideband approach allows for a limited demand for expensive radio frequency spectrum by using multiband signals and, through flexible signal design, straightforward implementation and integration with current and new generation telecommunications standards, such as 5G, are expected. In this project, we address multiband radio channel modelling, signal design for multiband ranging, estimation of the channel impulse response from a multiband signal, and multiband carrier-phase based ranging and positioning. Furthermore, a proof-of-principle hardware prototype test-bed will be developed for carrying out measurements and demonstrating the concept.The SuperGPS-2 project has started in Fall 2023 and the research team currently consists of two PhD-students supervised by two academic staff members at TU Delft. This project is actively supported by several partners from industry.Job descriptionThe SuperGPS-2 project is carried out jointly by the TU Delft Faculty of Electrical Engineering, Mathematics, and Computer Sciences (EEMCS) and the Faculty of Civil Engineering and Geosciences (CEG). The PostDoc should complement the current project-team, and will be appointed, for a 2.5 years term, at the former faculty. Next to research on radio signal processing and positioning, he/she will be leading the development of the proof-of-principle virtual ultra-wideband prototype test-bed for positioning and time-transfer, with the aim of a live-demonstration by the end of the project. Specifically, the PostDoc will supervise and contribute to the PhD students’ work on the prototype test-bed, and coordinate the actual demonstration.TU Delft offers an excellent and stimulating research environment with extensive available infrastructure and expertise.\n",
    "        Requirements         \n",
    "        Specific RequirementsCandidates are expected to have a PhD-degree in electrical engineering: telecommunications, signal processing or micro-electronics, excellent command of English (certified through a TOEFL or IELTS test), proven team-work competences, as well as good laboratory and programming competences (working with measurement equipment, electronics, and hardware). In addition, hardware programming skills (FPGA design, VHDL code development) are required and need to be proven.\"\"\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(new_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "# Realizar predicciones\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Obtener las predicciones de clase\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Convertir las predicciones de índices a etiquetas (si se desea)\n",
    "predicted_labels = [list(label_map.keys())[list(label_map.values()).index(pred)] for pred in predictions]\n",
    "\n",
    "print(predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
